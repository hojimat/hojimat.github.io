---
layout: cleanpost
title: Sensitivity Analysis
subtitle: local and global
lang: english
categ: note
description: Sobol here
tags: economics
---

There are different types: local and global. Lecture original: https://www.youtube.com/watch?v=wzTpoINJyBQ

### Local Sensitivity
- One variable at a time
- OFAT, OAT
- Kind of a partial derivative.
$$
	 \frac{\partial f(\cdot)}{\partial x_i} 
$$
- Linear regression is also here, but the $(x_i,y)$ needs to be normalized beforehand.

Disadvantages: no non-linearity, and no interaction between variables is captured.

### Global sensitivity
Is used if not trust the model too much. We have a model:
$$
	y = f(x),
$$
where $f(.)$ is a deterministic problem with probabilistic input.

We must know some kind of distribution of $x$ and the ranges of $x$ to have a well-defined sensitivity analysis. So, basically, any measure of sensitivity must give the relation:
$$
	\text{pdf}(y) \sim \text{pdf}(x)
$$

#### Derivative-based Global Sensitivity Measure
- Obtain $\beta = \frac{\partial f(x)}{x_i}$
- Calculate $\text{var} (\beta) = E(\beta^2) - E(\beta)^2$
- Both are calculated using integrals (as you would expected value)

#### Morris' One-At-A-Time
- no derivatives, just finite difference
- no interactions
- need more observations

#### Sobol method 
Kin of like non-linear ANOVA. Sobol decomposition:
$$
	y = f(x) = f_0 + \sum f_i(x_i) + \sum f_{ij}(x_i, x_j) + \sum f_{ijk}(x_i,x_j,x_k)...
$$
basically, a sum of "contributions" of a function on every variable, and every combination of variables. This only works for i.i.d. 

Do not confuse this with linear regression! Here $f_0$ is not an intercept when others are equal to zero, rather it is an intercept when others are __not changing__. So, we have:
$$
	\begin{align*}
	& E[y] = f_0,\\
	& E[y | x_i] = f_0 + f_i, \\
	& E[y | x_i, x_j]  = f_0 + f_i + f_j + f_{ij},\\
	& E[y | x_i, x_j, x_k] = f_0 + f_i + f_j + f_k + f_{ij} + f_{jk} + f_{ik}
	\end{align*}
$$
In the end, $y = E[y|x_1,x_2, ..., x_N]$. Now, let's take derivative of of both sides:
$$
	\text{var}(y) = \sum_i \text{var}(f_i) + \sum_{ij} \text{var}(f_{ij}) + ...
$$
and Sobol total index is $S_i$, where:
$$
	V_i = \text{var}(f_i) +\sum_j \text{var}(f_{ij}) + \sum_{jk} \text{var}(f_{ijk}) + ... \\
	S_i = \frac{V_i}{\text{var(y)}}
$$

So, in order to calculate Sobol sensitivity, we need to calculate a lot of these expected values (and consequently, the variances).

#### Quasi-Monte Carlo
__Low-discrepancy sequence__ is basically kind of like pseudorandom number, although they are neither random, nor pseudo-random - they are deterministic and are generated to minimize the clustering or spaces. In other words, you get something like a uniform random set, but it is even more uniform than uniform random. So, you will need less observations/draws to test more cases. So, if `runif(5)` might give you $(1,1,1,1,0.8)$, the `rlowdisc(5)` will give you $(1,0.8,0.6,0.4,0.2)$.
__Quasi-Monte Carlo__ is Monte Carlo which uses low-discrepancy sequence instead of pseudo-random draws.
It is used in financial simulations. Let's say you have 2 parameters - interest rate and inflation. To check all possible combinations of variables is a lot, and if you randomly draw pairs $(r,\pi)$ you will need 10,000 draws to get more or less uniform allocation. However, with quasi-monte-carlo you will only need 100 draws, and still cover most of the space. Some quasi-random algorithms are Latin hypercube and Sobol method. Latin can be generated once if you know the number exactly. Sobol can be generated repeatedly with the `next` function, where you continue generation.